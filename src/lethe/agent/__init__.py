"""Lethe Agent - Local agent with memory and tool execution.

Uses the local memory layer (LanceDB) and direct LLM calls.
Tools are just Python functions - no complex registration or approval loops.
"""

import asyncio
import logging
import os
from typing import Callable, Optional, Any

from lethe.config import Settings, get_settings
from lethe.memory import MemoryStore, AsyncLLMClient, LLMConfig, Hippocampus
from lethe.tools import get_all_tools, function_to_schema

logger = logging.getLogger(__name__)


class Agent:
    """Lethe agent with local memory and direct LLM calls.
    
    Architecture:
    - Memory: LanceDB (blocks, archival, messages)
    - LLM: OpenRouter (Kimi K2.5 by default)
    - Tools: Python functions, schemas auto-generated
    """
    
    def __init__(self, settings: Optional[Settings] = None):
        self.settings = settings or get_settings()
        
        # Initialize memory store
        self.memory = MemoryStore(
            data_dir=str(self.settings.memory_dir),
            workspace_dir=str(self.settings.workspace_dir),
            config_dir=str(self.settings.lethe_config_dir),
        )
        
        # Initialize LLM client (provider auto-detected from env vars)
        # Only pass model if explicitly set in env/settings (otherwise use provider default)
        llm_config = LLMConfig(
            provider=os.environ.get("LLM_PROVIDER", ""),  # Empty = auto-detect
            model=self.settings.llm_model,  # Empty = use provider default
            model_aux=self.settings.llm_model_aux,  # Empty = use provider default aux
            api_base=self.settings.llm_api_base,  # Custom API URL for local providers
            context_limit=self.settings.llm_context_limit,
        )
        
        # Load system prompt from config
        system_prompt = self._build_system_prompt()
        
        # Get memory context
        memory_context = self.memory.get_context_for_prompt()
        
        self.llm = AsyncLLMClient(
            config=llm_config,
            system_prompt=system_prompt,
            memory_context=memory_context,
        )
        
        # Initialize hippocampus with LLM functions (analyzer + summarizer use aux model)
        hippocampus_enabled = os.environ.get("HIPPOCAMPUS_ENABLED", "true").lower() == "true"
        self.hippocampus = Hippocampus(
            self.memory, 
            summarizer=self._summarize_memories,
            analyzer=self._summarize_memories,  # Same aux model for analysis
            enabled=hippocampus_enabled,
        )
        
        # Add internal memory tools
        self._add_memory_tools()
        
        # Add external tools (file, bash, browser)
        self.llm.add_tools(get_all_tools())
        
        # Note: call await agent.initialize() after creation to load message history
        self._initialized = False
        
        logger.info(f"Agent initialized with model {self.settings.llm_model}")
    
    async def initialize(self):
        """Async initialization - load message history with summarization."""
        if self._initialized:
            return
        await self._load_message_history()
        self._initialized = True
    
    async def _load_message_history(self):
        """Load recent message history into LLM context.
        
        Uses configurable two-tier loading:
        1. Load last N messages verbatim (LLM_MESSAGES_LOAD, default 20)
        2. Summarize M messages before that (LLM_MESSAGES_SUMMARIZE, default 100)
        """
        load_count = self.settings.llm_messages_load
        summarize_count = self.settings.llm_messages_summarize
        total_needed = load_count + summarize_count
        
        # Get all messages we need
        all_messages = self.memory.messages.get_recent(total_needed)
        if not all_messages:
            return
        
        # Reverse to chronological order (oldest first)
        all_messages = list(reversed(all_messages))
        
        # Split into messages to summarize and messages to load verbatim
        if len(all_messages) > load_count:
            to_summarize = all_messages[:-load_count]
            to_load = all_messages[-load_count:]
        else:
            to_summarize = []
            to_load = all_messages
        
        # Summarize older messages if any
        if to_summarize:
            summary = await self._summarize_message_history(to_summarize)
            if summary:
                self.llm.context.summary = summary
                logger.info(f"Summarized {len(to_summarize)} older messages")
        
        # Load recent messages verbatim
        if to_load:
            self.llm.load_messages(to_load)
            logger.info(f"Loaded {len(to_load)} messages from history")
    
    async def _summarize_message_history(self, messages: list) -> str:
        """Summarize a list of messages using aux model."""
        # Format messages for summarization
        formatted = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if isinstance(content, list):
                content = " ".join(p.get("text", "") for p in content if isinstance(p, dict))
            formatted.append(f"{role}: {content[:500]}")
        
        messages_text = "\n".join(formatted)
        
        prompt = f"""Summarize this conversation history concisely. Preserve key facts, decisions, and context.

{messages_text}

Summary:"""
        
        try:
            summary = await self.llm.complete(prompt)  # Uses aux model
            return summary.strip() if summary else ""
        except Exception as e:
            logger.warning(f"Failed to summarize history: {e}")
            return ""
    
    def _build_system_prompt(self) -> str:
        """Build system prompt from persona block in workspace."""
        # Read from workspace memory blocks (not config seeds)
        persona_block = self.memory.blocks.get_by_label("persona")
        
        if persona_block:
            return persona_block["value"]
        
        return "You are Lethe, an autonomous AI assistant."
    
    async def _summarize_memories(self, prompt: str) -> str:
        """Summarize memories using LLM (for hippocampus)."""
        return await self.llm.complete(prompt)
    
    def _add_memory_tools(self):
        """Add internal memory management tools."""
        # Simple tool definitions - schemas auto-generated from docstrings
        
        def memory_read(label: str) -> str:
            """Read a memory block by label.
            
            Args:
                label: Block label to read (e.g., 'persona', 'human', 'project')
            """
            block = self.memory.blocks.get_by_label(label)
            if block:
                return f"[{label}]\n{block['value']}"
            return f"Block '{label}' not found"
        
        def memory_update(label: str, value: str) -> str:
            """Update a memory block's value.
            
            Args:
                label: Block label to update
                value: New value for the block
            """
            try:
                if self.memory.blocks.update(label, value=value):
                    self.llm.update_memory_context(self.memory.get_context_for_prompt())
                    return f"Updated block '{label}'"
                return f"Block '{label}' not found"
            except Exception as e:
                return f"Error: {e}"
        
        def memory_append(label: str, text: str) -> str:
            """Append text to a memory block.
            
            Args:
                label: Block label to append to
                text: Text to append
            """
            try:
                if self.memory.blocks.append(label, text):
                    self.llm.update_memory_context(self.memory.get_context_for_prompt())
                    return f"Appended to block '{label}'"
                return f"Block '{label}' not found"
            except Exception as e:
                return f"Error: {e}"
        
        def archival_search(query: str, limit: int = 10) -> str:
            """Search long-term archival memory.
            
            Args:
                query: Search query
                limit: Max results (default 10)
            """
            results = self.memory.archival.search(query, limit=limit)
            if not results:
                return "No results found"
            
            output = []
            for i, r in enumerate(results, 1):
                output.append(f"{i}. [{r['score']:.2f}] {r['text']}")
            return "\n".join(output)
        
        def archival_insert(text: str) -> str:
            """Store information in long-term archival memory.
            
            Args:
                text: Text to store
            """
            mem_id = self.memory.archival.add(text)
            return f"Stored in archival memory (id: {mem_id})"
        
        def conversation_search(query: str, limit: int = 10, role: str = "") -> str:
            """Search conversation history.
            
            Args:
                query: Search query
                limit: Max results (default 10)
                role: Filter by role (user, assistant) - optional
            """
            if role:
                results = self.memory.messages.search_by_role(query, role, limit=limit)
            else:
                results = self.memory.messages.search(query, limit=limit)
            
            if not results:
                return "No matching messages found"
            
            output = []
            for r in results:
                timestamp = r['created_at'][:16].replace('T', ' ')
                content = r['content'][:200] + "..." if len(r['content']) > 200 else r['content']
                output.append(f"[{timestamp}] {r['role']}: {content}")
            
            return f"Found {len(results)} messages:\n\n" + "\n\n".join(output)
        
        def send_image(file_path: str) -> dict:
            """Send an image to the user via Telegram.
            
            Use this to send screenshots, generated images, or any image file.
            The image will be sent in the correct order with your response.
            
            Args:
                file_path: Path to the image file to send
            """
            import os
            if not os.path.exists(file_path):
                return {"status": "error", "message": f"File not found: {file_path}"}
            
            # Return with _image_attachment to trigger on_image callback
            return {
                "status": "ok",
                "message": f"Image queued: {file_path}",
                "_image_attachment": {"path": file_path}
            }
        
        # Add all memory tools
        for func in [memory_read, memory_update, memory_append, 
                     archival_search, archival_insert, conversation_search,
                     send_image]:
            self.llm.add_tool(func)
    
    def add_tool(self, func: Callable):
        """Add a custom tool function."""
        self.llm.add_tool(func)
    
    async def chat(
        self,
        message: str,
        on_message: Optional[Callable[[str], Any]] = None,
        on_image: Optional[Callable[[str], Any]] = None,
        use_hippocampus: bool = True,
    ) -> str:
        """Send a message and get a response.
        
        Args:
            message: User message
            on_message: Optional callback for intermediate messages
            on_image: Optional callback for image attachments (screenshots)
            use_hippocampus: Whether to augment with recalled memories (default True)
            
        Returns:
            Final assistant response
        """
        # Store user message in history (original, without recall)
        self.memory.messages.add("user", message)
        
        # Augment message with hippocampus recall (unless disabled)
        if use_hippocampus:
            recent = self.memory.messages.get_recent(10)
            augmented_message = await self.hippocampus.augment_message(message, recent)
        else:
            augmented_message = message
        
        # Get response from LLM (handles tool calls internally)
        response = await self.llm.chat(augmented_message, on_message=on_message, on_image=on_image)
        
        # Store assistant response in history
        self.memory.messages.add("assistant", response)
        
        return response
    
    async def heartbeat(self, message: str) -> str:
        """Process heartbeat with minimal context and aux model.
        
        Uses lightweight context (no full identity, limited history) and
        aux model for cost efficiency.
        
        Args:
            message: Heartbeat message
            
        Returns:
            Response string
        """
        return await self.llm.heartbeat(message)
    
    async def close(self):
        """Clean up resources."""
        await self.llm.close()
    
    def get_stats(self) -> dict:
        """Get agent statistics."""
        return {
            "model": self.settings.llm_model,
            "memory_blocks": len(self.memory.blocks.list_blocks()),
            "archival_memories": self.memory.archival.count(),
            "message_history": self.memory.messages.count(),
            "tools": len(self.llm._tools),
            "llm": self.llm.get_context_stats(),
        }
    
    def refresh_memory_context(self):
        """Refresh LLM memory context from current blocks."""
        self.llm.update_memory_context(self.memory.get_context_for_prompt())
